{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to statistically evaluate your data from ensemble runs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given, that you whent through 02_tutorial, there should be some results to work with already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0o11_0o1_0o005_2o0_1000_s4.pkl', '0o1_0o1_0o005_2o0_1000_s4.pkl', '0o11_0o1_0o01_1o0_1000_s2.pkl', '0o09_0o1_0o1_2o0_1000_s0.pkl', '0o1_0o1_0o01_2o0_1000_s4.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "SAVE_PATH_RAW = \"./dummy/pymofatutorial\"\n",
    "print os.listdir(SAVE_PATH_RAW)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to do some MPI accelerated statistical analysis with this data using the resave routine of the experiment_handle class from pymofa.\n",
    "Therefore, again, we need to start an *ipcontroller* and some *engines* for ipyparallel\n",
    "\n",
    "$ipcluster\n",
    "\n",
    "$mpirun-n [number of engines] ipengine --mpi=mpi4py\n",
    "\n",
    "and connect to them with the IPython kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "c = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to to setup the experiment handle from the previous experiment again, only that this time, we will pass the experiment handle the optional path to create a folder for results (different from the one for the raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "0\n",
      "[stdout:1] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "0\n",
      "[stdout:2] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "0\n",
      "[stdout:3] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "0\n",
      "[stdout:4] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "3\n",
      "[stdout:5] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "2\n",
      "[stdout:6] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "0\n",
      "[stdout:7] \n",
      "{0: 'prey_birth_rate', 1: 'coupling', 2: 'predator_death_rate', 3: 'initial_pop', 4: 'time_length'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "# imports\n",
    "from pymofa.experiment_handling import experiment_handling as eh\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "# import cPickle\n",
    "\n",
    "\n",
    "#Definingh the experiment execution function\n",
    "#it gets paramater you want to investigate, plus `filename` as the last parameter\n",
    "def RUN_FUNC(prey_birth_rate, coupling, predator_death_rate, initial_pop, time_length,\n",
    "             filename):\n",
    "    \"\"\"Dummy RUN_FUNC just to make the INDEX (below) work.\"\"\"\n",
    "    \n",
    "    exit_status = 42\n",
    "    randomvarname = 'bla'\n",
    "\n",
    "    return exit_status \n",
    "\n",
    "# Path where to Store the simulated Data\n",
    "SAVE_PATH_RAW = \"./dummy/pymofatutorial/\"\n",
    "\n",
    "#path to folder for results of statistical evaluation\n",
    "SAVE_PATH_RES = \"./dummy/stateval_results/\"\n",
    "\n",
    "# Parameter combinations to investiage\n",
    "prey_birth_rate = [0.09, 0.1, 0.11]\n",
    "coupling = [0.1]\n",
    "predator_death_rate = [0.005, 0.01, 0.05, 0.1]\n",
    "initial_pop = [1.0, 2.0]\n",
    "time_length = [1000]\n",
    "\n",
    "PARAM_COMBS = list(it.product(prey_birth_rate, coupling, predator_death_rate, initial_pop, time_length))\n",
    "\n",
    "# Sample Size\n",
    "SAMPLE_SIZE = 5\n",
    "\n",
    "# INDEX \n",
    "INDEX = {i: RUN_FUNC.func_code.co_varnames[i] for i in xrange(RUN_FUNC.__code__.co_argcount-1)}\n",
    "print INDEX\n",
    "\n",
    "# initiate handle instance with experiment variables\n",
    "handle = eh(SAMPLE_SIZE, PARAM_COMBS, INDEX, SAVE_PATH_RAW, SAVE_PATH_RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resave routine of this experiment handle requires a dictionary of callables and a filename for the results as inputs. The experiment handle keeps track of the simulation results internally in a list of filenames that is required as an input to the callables passed to the resave routine.\n",
    "\n",
    "Note, that the callables are designed to handle Pandas Dataframes. To be more exact, they load the Dataframes for each list of filenames in a list. Then they concatenate all the Dataframes in the list together in one dataframe. Then the groupby routine of the Dataframe class groups all rows according to their index value on the first level (the timestep in our case) and then applies either a 'mean' or a 'standard error of the mean' estimator to these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "processing  stateval_results\n",
      "under operators  ['sem', 'mean']\n",
      "Post-processing done\n",
      "[stdout:1] \n",
      "processing  stateval_results\n",
      "under operators  ['sem', 'mean']\n",
      "Post-processing done\n",
      "[stdout:2] \n",
      "processing  stateval_results\n",
      "under operators  ['sem', 'mean']\n",
      "Post-processing done\n",
      "[stdout:3] \n",
      "processing  stateval_results\n",
      "under operators  ['sem', 'mean']\n",
      "Post-processing done\n",
      "[stdout:6] \n",
      "processing  stateval_results\n",
      "under operators  ['sem', 'mean']\n",
      "\r",
      "Post-processing 3 ... [2%] \r",
      "Post-processing 1 ... [4%] \r",
      "Post-processing 2 ... [6%] \r",
      "Post-processing 3 ... [8%] \r",
      "Post-processing 3 ... [10%] \r",
      "Post-processing 1 ... [12%] \r",
      "Post-processing 2 ... [15%] \r",
      "Post-processing 3 ... [17%] \r",
      "Post-processing 2 ... [19%] \r",
      "Post-processing 1 ... [21%] \r",
      "Post-processing 3 ... [23%] \r",
      "Post-processing 3 ... [25%] \r",
      "Post-processing 1 ... [27%] \r",
      "Post-processing 2 ... [28%] \r",
      "Post-processing 3 ... [31%] \r",
      "Post-processing 3 ... [33%] \r",
      "Post-processing 1 ... [35%] \r",
      "Post-processing 2 ... [38%] \r",
      "Post-processing 3 ... [40%] \r",
      "Post-processing 3 ... [42%] \r",
      "Post-processing 2 ... [44%] \r",
      "Post-processing 1 ... [46%] \r",
      "Post-processing 3 ... [48%] \r",
      "Post-processing 1 ... [50%] \r",
      "Post-processing 3 ... [52%] \r",
      "Post-processing 2 ... [54%] \r",
      "Post-processing 3 ... [56%] \r",
      "Post-processing 1 ... [57%] \r",
      "Post-processing 2 ... [60%] \r",
      "Post-processing 3 ... [62%] \r",
      "Post-processing 2 ... [65%] \r",
      "Post-processing 3 ... [67%] \r",
      "Post-processing 1 ... [69%] \r",
      "Post-processing 2 ... [71%] \r",
      "Post-processing 3 ... [73%] \r",
      "Post-processing 1 ... [75%] \r",
      "Post-processing 2 ... [77%] \r",
      "Post-processing 3 ... [79%] \r",
      "Post-processing 1 ... [81%] \r",
      "Post-processing 3 ... [83%] \r",
      "Post-processing 1 ... [85%] \r",
      "Post-processing 2 ... [88%] \r",
      "Post-processing 3 ... [90%] \r",
      "Post-processing 2 ... [92%] \r",
      "Post-processing 1 ... [94%] \r",
      "Post-processing 3 ... [96%] \r",
      "Post-processing 2 ... [98%] \n",
      "\r",
      "Post-processing 1 ... [100%] Post-processing done\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "filename = \"stateval_results\"\n",
    "\n",
    "def sem(fnames):\n",
    "    \"\"\"calculate the standard error of the mean for the data in the files\n",
    "    that are in the list of fnames\n",
    "    \n",
    "    Parameter:\n",
    "    ----------\n",
    "    fnames: string\n",
    "        list of strings of filenames containing simulation results\n",
    "    Returns:\n",
    "    sem: float\n",
    "        Standard error of the mean of the data in the files specified\n",
    "        by the list of fnames\n",
    "    \"\"\"\n",
    "    import scipy.stats as st\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    return pd.concat([np.load(f) for f in fnames]).groupby(level=0).mean()\n",
    "    \n",
    "\n",
    "#callables can be functions, lambda expressions etc...\n",
    "EVA = {\"sem\": sem,\n",
    "        \"mean\": lambda fnames: pd.concat([np.load(f) for f in fnames]).groupby(level=0).mean()}\n",
    "\n",
    "handle.resave(EVA, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
